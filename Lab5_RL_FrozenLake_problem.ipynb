{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP514NZq/zgvejQhMpSLFgP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9EQIEcI_2U2w","executionInfo":{"status":"ok","timestamp":1715139834219,"user_tz":-330,"elapsed":599,"user":{"displayName":"Shreya Shree S","userId":"02564406637729607169"}},"outputId":"f498416b-affb-4744-f8a8-8c182ecccbe8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Frozen Lake\n","[0 0 0 0]\n","[ 0 -1  0 -1]\n","[ 0  0  0 -1]\n","[-1  0  0  1]\n","\n","Training complete!\n","Q-values:\n","[[[ 0.53138786  0.47351393  0.59049     0.53143569]\n","  [ 0.4778186   0.42616254 -1.          0.531441  ]\n","  [ 0.43003674  0.34832951  0.38703307  0.4782969 ]\n","  [ 0.38354628  0.         -1.          0.43046721]]\n","\n"," [[ 0.53129698 -1.          0.6561      0.5845851 ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.43046721 -1.          0.80839539 -1.        ]\n","  [ 0.          0.          0.          0.        ]]\n","\n"," [[ 0.59043095  0.729      -0.9999      0.65609934]\n","  [-1.          0.65609934  0.81        0.65603439]\n","  [ 0.34867844 -0.99        0.9         0.729     ]\n","  [ 0.          0.          0.          0.        ]]\n","\n"," [[ 0.          0.          0.          0.        ]\n","  [ 0.72899993  0.9         0.80999992 -1.        ]\n","  [ 0.80999846  1.          0.9         0.80999919]\n","  [ 0.          0.          0.          0.        ]]]\n","Final path from Start state [0, 0]: [[0, 0], [1, 0], [2, 0], [2, 1], [3, 1], [3, 2], [3, 3]]\n"]}],"source":["import numpy as np\n","\n","# Define the environment (Frozen Lake)\n","environment_rows = 4\n","environment_columns = 4\n","\n","# Create a 3D numpy array to hold the current Q-values for each state and action pair\n","q_values = np.zeros((environment_rows, environment_columns, 4))  # 4 actions: up, right, down, left\n","\n","# Define actions\n","actions = ['up', 'right', 'down', 'left']\n","\n","# Define rewards for each state\n","rewards = np.array([[0, 0, 0, 0],\n","                    [0, -1, 0, -1],\n","                    [0, 0, 0, -1],\n","                    [-1, 0, 0, 1]])\n","\n","# Define a function to check if a state is terminal\n","def is_terminal_state(row_index, column_index):\n","    return rewards[row_index, column_index] != 0\n","\n","# Define a function to choose a starting location\n","def get_starting_location():\n","    return np.random.randint(environment_rows), np.random.randint(environment_columns)\n","\n","# Define an epsilon-greedy action selection function\n","def get_next_action(current_row_index, current_column_index, epsilon):\n","    if np.random.random() < epsilon:\n","        return np.argmax(q_values[current_row_index, current_column_index])\n","    else:\n","        return np.random.randint(4)\n","\n","# Define a function to get the next location based on the action taken\n","def get_next_location(current_row_index, current_column_index, action_index):\n","    if actions[action_index] == 'up':\n","        return max(0, current_row_index - 1), current_column_index\n","    elif actions[action_index] == 'right':\n","        return current_row_index, min(environment_columns - 1, current_column_index + 1)\n","    elif actions[action_index] == 'down':\n","        return min(environment_rows - 1, current_row_index + 1), current_column_index\n","    elif actions[action_index] == 'left':\n","        return current_row_index, max(0, current_column_index - 1)\n","\n","# Define a function to find the path from a given starting location\n","def get_path(start_row_index, start_column_index):\n","    if is_terminal_state(start_row_index, start_column_index):\n","        return []\n","    else:\n","        current_row_index, current_column_index = start_row_index, start_column_index\n","        shortest_path = []\n","        shortest_path.append([current_row_index, current_column_index])\n","        while not is_terminal_state(current_row_index, current_column_index):\n","            action_index = get_next_action(current_row_index, current_column_index, 1.)  # Choose the best action\n","            current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n","            shortest_path.append([current_row_index, current_column_index])\n","        return shortest_path\n","\n","# Training parameters\n","epsilon = 0.9\n","discount_factor = 0.9\n","learning_rate = 0.9\n","num_episodes = 1000\n","\n","# Training loop\n","for episode in range(num_episodes):\n","    row_index, column_index = get_starting_location()\n","    while not is_terminal_state(row_index, column_index):\n","        action_index = get_next_action(row_index, column_index, epsilon)\n","        old_row_index, old_column_index = row_index, column_index\n","        row_index, column_index = get_next_location(row_index, column_index, action_index)\n","        reward = rewards[row_index, column_index]\n","        old_q_value = q_values[old_row_index, old_column_index, action_index]\n","        temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n","        new_q_value = old_q_value + (learning_rate * temporal_difference)\n","        q_values[old_row_index, old_column_index, action_index] = new_q_value\n","\n","print(\"Frozen Lake\")\n","for row in rewards:\n","  print(row)\n","print()\n","print('Training complete!')\n","print('Q-values:')\n","print(q_values)\n","print('Final path from Start state [0, 0]:', get_path(0,0))"]},{"cell_type":"code","source":[],"metadata":{"id":"pbbWhwao6k3H"},"execution_count":null,"outputs":[]}]}