{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPu7r4CrE/AJ6ixGFS0V1xL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install gym\n","!pip install git+https://github.com/mimoralea/gym-walk#egg=gym-walk\n","import gym, gym_walk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uznhNZ9hRmQr","executionInfo":{"status":"ok","timestamp":1712891285955,"user_tz":-330,"elapsed":13234,"user":{"displayName":"Shreya Shree S","userId":"02564406637729607169"}},"outputId":"4c46fcf4-b098-45f0-bf1c-6a87106f47ff"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n","Collecting gym-walk\n","  Cloning https://github.com/mimoralea/gym-walk to /tmp/pip-install-3l9zspxa/gym-walk_bfd0a2c83586444c8026727c0dcf9711\n","  Running command git clone --filter=blob:none --quiet https://github.com/mimoralea/gym-walk /tmp/pip-install-3l9zspxa/gym-walk_bfd0a2c83586444c8026727c0dcf9711\n","  Resolved https://github.com/mimoralea/gym-walk to commit 5999016267d6de2f5a63307fb00dfd63de319ac1\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from gym-walk) (0.25.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-walk) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-walk) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->gym-walk) (0.0.8)\n","Building wheels for collected packages: gym-walk\n","  Building wheel for gym-walk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym-walk: filename=gym_walk-0.0.2-py3-none-any.whl size=4053 sha256=c7a02bcd2204a38eaebc01301e3ef78df1ad78089ff812db25e5ab44213385b5\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-mpf8qsms/wheels/24/fe/c4/0cbc7511d29265bad7e28a09311db3f87f0cafba74af54d530\n","Successfully built gym-walk\n","Installing collected packages: gym-walk\n","Successfully installed gym-walk-0.0.2\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from IPython import display\n","import time"],"metadata":{"id":"lvVtMYkdWbTl","executionInfo":{"status":"ok","timestamp":1712892526577,"user_tz":-330,"elapsed":499,"user":{"displayName":"Shreya Shree S","userId":"02564406637729607169"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### Bandit Walk"],"metadata":{"id":"YiHMxvPBRAR3"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"DuH_Sf4pOv5M","executionInfo":{"status":"ok","timestamp":1712891470735,"user_tz":-330,"elapsed":5,"user":{"displayName":"Shreya Shree S","userId":"02564406637729607169"}}},"outputs":[],"source":["#BW\n","P = {\n","    0:{\n","        0: [(1.0,0,0.0, True)],\n","        1: [(1,0,0,0.0, True)]\n","    },\n","    1:{\n","         0: [(1.0,0,0.0, True)],\n","         1: [(1.0,2,1.0, True)]\n","    },\n","    2:{\n","        0: [(1.0,2,0.0, True)],\n","        1: [(1,0,2,0.0, True)]\n","    }\n","}\n","P1 = gym.make('BanditWalk-v0').env.P"]},{"cell_type":"code","source":["import numpy as np\n","\n","class TDAgent:\n","    def __init__(self, num_states, alpha=0.1, gamma=0.99, epsilon=0.1):\n","        self.num_states = num_states\n","        self.alpha, self.gamma, self.epsilon = alpha, gamma, epsilon\n","        self.values = np.zeros(num_states)\n","\n","    def choose_action(self, state):\n","        return np.random.choice([0, 1]) if np.random.rand() < self.epsilon else np.argmax(self.values[state])\n","\n","    def update_value(self, state, reward, next_state):\n","        td_target = reward + self.gamma * self.values[next_state]\n","        self.values[state] += self.alpha * (td_target - self.values[state])\n","\n","# Environment transition probabilities dictionary\n","P = {\n","    0: {\n","        0: [(1.0, 0, 0.0, True)],\n","        1: [(1, 0, 0, 0.0, True)]\n","    },\n","    1: {\n","        0: [(1.0, 0, 0.0, True)],\n","        1: [(1.0, 2, 1.0, True)]\n","    },\n","    2: {\n","        0: [(1.0, 2, 0.0, True)],\n","        1: [(1, 0, 2, 0.0, True)]\n","    }\n","}\n","\n","# Parameters\n","num_episodes, num_states = 1000, len(P)\n","\n","# Initialize\n","agent = TDAgent(num_states)\n","\n","# Training loop\n","for _ in range(num_episodes):\n","    state, done = 1, False  # Start from state 1\n","    while not done:\n","        action = agent.choose_action(state)\n","        prob, next_state, reward, done = P[state][action][0]  # Get next state and reward from transition probabilities\n","        agent.update_value(state, reward, next_state)\n","        state = next_state\n","\n","# After training\n","print(\"Learned values:\")\n","print(agent.values)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YGKJ1jfuU9gA","executionInfo":{"status":"ok","timestamp":1712893226439,"user_tz":-330,"elapsed":425,"user":{"displayName":"Shreya Shree S","userId":"02564406637729607169"}},"outputId":"6777c216-a14e-41f1-c06c-5965efea3737"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Learned values:\n","[0.         0.02580032 0.        ]\n"]}]},{"cell_type":"markdown","source":["### Bandit Slippery Walk"],"metadata":{"id":"0mamRjV3SBdl"}},{"cell_type":"code","source":["#BSW\n","P = {\n","    0:{\n","        0: [(1.0,0,0.0, True)],\n","        1: [(1,0,0,0.0, True)]\n","    },\n","    1:{\n","         0: [(0.8,0,0.0, True), (0.2,2,1.0, True)],\n","         1: [(0.8,2,1.0, True), (0.2,0,0.0, True)]\n","    },\n","    2:{\n","        0: [(1.0,2,0.0, True)],\n","        1: [(1,0,2,0.0, True)]\n","    }\n","}\n","P2 = gym.make('BanditSlipperyWalk-v0').env.P"],"metadata":{"id":"fMaaGDaURXyT","executionInfo":{"status":"ok","timestamp":1712891476229,"user_tz":-330,"elapsed":2,"user":{"displayName":"Shreya Shree S","userId":"02564406637729607169"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","class TDAgent:\n","    def __init__(self, num_states, alpha=0.1, gamma=0.99, epsilon=0.1):\n","        self.num_states = num_states\n","        self.alpha, self.gamma, self.epsilon = alpha, gamma, epsilon\n","        self.values = np.zeros(num_states)\n","\n","    def choose_action(self, state):\n","        return np.random.choice([0, 1]) if np.random.rand() < self.epsilon else np.argmax(self.values[state])\n","\n","    def update_value(self, state, reward, next_state):\n","        td_target = reward + self.gamma * self.values[next_state]\n","        self.values[state] += self.alpha * (td_target - self.values[state])\n","\n","# Bandit Slippery walk environment transition probabilities dictionary\n","P = {\n","    0: {\n","        0: [(1.0, 0, 0.0, True)],\n","        1: [(1.0, 0, 0.0, True)]\n","    },\n","    1: {\n","        0: [(0.8, 0, 0.0, True), (0.2, 2, 1.0, True)],\n","        1: [(0.8, 2, 1.0, True), (0.2, 0, 0.0, True)]\n","    },\n","    2: {\n","        0: [(1.0, 2, 0.0, True)],\n","        1: [(1, 0, 2, 0.0, True)]\n","    }\n","}\n","\n","# Parameters\n","num_episodes, num_states = 1000, len(P)\n","\n","# Initialize\n","agent = TDAgent(num_states)\n","\n","# Training loop\n","for _ in range(num_episodes):\n","    state, done = 1, False  # Start from state 1\n","    while not done:\n","        action = agent.choose_action(state)\n","        transitions = P[state][action]\n","        prob, next_state, reward, done = transitions[np.random.choice(len(transitions))]\n","        agent.update_value(state, reward, next_state)\n","        state = next_state\n","\n","# After training\n","print(\"Learned values:\")\n","print(agent.values)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S_pS2QrkSKOu","executionInfo":{"status":"ok","timestamp":1712893333936,"user_tz":-330,"elapsed":465,"user":{"displayName":"Shreya Shree S","userId":"02564406637729607169"}},"outputId":"8fbb5771-e421-4bd2-9349-e30c926725f0"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Learned values:\n","[0.        0.3420852 0.       ]\n"]}]},{"cell_type":"markdown","source":["### Frozen Lake"],"metadata":{"id":"brqgtDlbauf8"}},{"cell_type":"code","source":["import numpy as np\n","\n","class TDAgent:\n","    def __init__(self, num_states, alpha=0.1, gamma=0.99, epsilon=0.1):\n","        self.num_states = num_states\n","        self.alpha, self.gamma, self.epsilon = alpha, gamma, epsilon\n","        self.values = np.random.rand(num_states)  # Initialize with random values\n","\n","    def choose_action(self, state):\n","        return np.random.choice([0, 1, 2, 3]) if np.random.rand() < self.epsilon else np.argmax(self.values[state])\n","\n","    def update_value(self, state, reward, next_state):\n","        td_target = reward + self.gamma * self.values[next_state]\n","        self.values[state] += self.alpha * (td_target - self.values[state])\n","\n","# Frozen Lake environment transition probabilities dictionary\n","P = {\n","    0:{\n","        0: [(0.33,0,0.0,False),(0.33,0,0.0,False),(0.33,4,0.0,False)],\n","        1: [(0.33,4,0.0,False),(0.33,0,0.0,False),(0.33,1,0.0,False)],\n","        2: [(0.33,1,0.0,False),(0.33,4,0.0,False),(0.33,0,0.0,False)],\n","        3: [(0.33,0,0.0,False),(0.33,0,0.0,False),(0.33,1,0.0,False)]\n","    },\n","    1:{\n","        0: [(0.33,0,0.0,False),(0.33,1,0.0,False),(0.33,5,0.0,True)],\n","        1: [(0.33,0,0.0,False),(0.33,5,0.0,True),(0.33,2,0.0,False)],\n","        2: [(0.33,5,0.0,True),(0.33,2,0.0,False),(0.33,1,0.0,False)],\n","        3: [(0.33,0,0.0,False),(0.33,1,0.0,False),(0.33,2,0.0,False)]\n","    },\n","    2:{\n","        0: [(0.33,1,0.0,False),(0.33,2,0.0,False),(0.33,6,0.0,False)],\n","        1: [(0.33,1,0.0,False),(0.33,3,0.0,False),(0.33,6,0.0,False)],\n","        2: [(0.33,2,0.0, False),(0.33,3,0.0, False),(0.33,6,0.0,False)],\n","        3: [(0.33,1,0.0, False),(0.33,2,0.0,False),(0.33,3,0.0,False)]\n","    },\n","    3:{\n","        0: [(0.33,2,0.0,False),(0.33,3,0.0,False),(0.33,7,0.0,True)],\n","        1: [(0.33,7,0.0,True),(0.33,2,0.0,False),(0.33,3,0.0,False)],\n","        2: [(0.33,3,0.0,False),(0.33,3,0.0,False),(0.33,7,0.0,True)],\n","        3: [(0.33,2,0.0,False),(0.33,3,0.0,False),(0.33,3,0.0,False)]\n","    },\n","    4:{\n","        0: [(0.33,4,0.0,False),(0.33,0,0.0,False),(0.33,8,0.0,False)],\n","        1: [(0.33,4,0.0,False),(0.33,8,0.0,False),(0.33,5,0.0,True)],\n","        2: [(0.33,0,0.0, False),(0.33,5,0.0,True),(0.33,8,0.0,False)],\n","        3: [(0.33,0,0.0, False),(0.33,4,0.0,False),(0.33,5,0.0,True)]\n","    },\n","    5:{\n","        0: [(1,5,0.0,True)],\n","        1: [(1,5,0.0,True)],\n","        2: [(1,5,0.0,True)],\n","        3: [(1,5,0.0,True)]\n","    },\n","    6:{\n","        0: [(0.33,5,0.0,True),(0.33,2,0.0,False),(0.33,10,0.0,False)],\n","        1: [(0.33,10,0.0,False),(0.33,5,0.0,True),(0.33,7,0.0,True)],\n","        2: [(0.33,10,0.0,False),(0.33,2,0.0,False),(0.33,7,0.0,True)],\n","        3: [(0.33,2,0.0, False),(0.33,5,0.0,True),(0.33,7,0.0,True)]\n","    },\n","    7:{\n","        0: [(1,7,0.0,True)],\n","        1: [(1,7,0.0,True)],\n","        2: [(1,7,0.0,True)],\n","        3: [(1,7,0.0,True)]\n","    },\n","    8:{\n","        0: [(0.33,4,0.0,False),(0.33,8,0.0,False),(0.33,12,0.0,True)],\n","        1: [(0.33,8,0.0,False),(0.33,12,0.0,True),(0.33,9,0.0,False)],\n","        2: [(0.33,4,0.0,False),(0.33,9,0.0,False),(0.33,12,0.0,True)],\n","        3: [(0.33,8,0.0,False),(0.33,4,0.0,False),(0.33,9,0.0,False)]\n","    },\n","    9:{\n","        0: [(0.33,5,0.0,True),(0.33,8,0.0,False),(0.33,13,0.0,False)],\n","        1: [(0.33,8,0.0,False),(0.33,13,0.0,False),(0.33,10,0.0,False)],\n","        2: [(0.33,5,0.0,True),(0.33,10,0.0,False),(0.33,13,0.0,False)],\n","        3: [(0.33,8,0.0,False),(0.33,5,0.0,True),(0.33,10,0.0,False)]\n","    },\n","    10:{\n","        0: [(0.33,6,0.0,False),(0.33,9,0.0,False),(0.33,14,0.0,False)],\n","        1: [(0.33,9,0.0,False),(0.33,14,0.0,False),(0.33,11,0.0,True)],\n","        2: [(0.33,14,0.0,False),(0.33,11,0.0,True),(0.33,6,0.0,False)],\n","        3: [(0.33,6,0.0,False),(0.33,9,0.0,False),(0.33,11,0.0,True)]\n","    },\n","    11:{\n","        0: [(1,11,0.0,True)],\n","        1: [(1,11,0.0,True)],\n","        2: [(1,11,0.0,True)],\n","        3: [(1,11,0.0,True)]\n","    },\n","    12:{\n","        0: [(1,12,0.0,True)],\n","        1: [(1,12,0.0,True)],\n","        2: [(1,12,0.0,True)],\n","        3: [(1,12,0.0,True)]\n","    },\n","    13:{\n","        0: [(0.33,12,0.0,True),(0.33,13,0.0,False),(0.33,9,0.0,False)],\n","        1: [(0.33,14,0.0,False),(0.33,13,0.0,False),(0.33,12,0.0,True)],\n","        2: [(0.33,13,0.0,False),(0.33,14,0.0,False),(0.33,9,0.0,False)],\n","        3: [(0.33,12,0.0,True),(0.33,9,0.0,False),(0.33,14,0.0,False)]\n","    },\n","    14:{\n","        0: [(0.33,10,0.0,False),(0.33,13,0.0,False),(0.33,14,0.0,False)],\n","        1: [(0.33,13,0.0,False),(0.33,14,0.0,False),(0.33,15,1.0,True)],\n","        2: [(0.33,14,0.0,False),(0.33,15,1.0,True),(0.33,10,0.0,False)],\n","        3: [(0.33,15,1.0,True),(0.33,10,0.0,False),(0.33,13,0.0,False)]\n","    },\n","    15:{\n","        0: [(1.0, 15, 0.0, True)],\n","        1: [(1.0, 15, 0.0, True)],\n","        2: [(1.0, 15, 0.0, True)],\n","        3: [(1.0, 15, 0.0, True)]\n","    }\n","}\n","# Parameters\n","num_episodes, num_states = 20, len(P)\n","\n","# Initialize\n","agent = TDAgent(num_states)\n","\n","# Training loop\n","for _ in range(num_episodes):\n","    state, done = 0, False  # Start from state 0\n","    while not done:\n","        action = agent.choose_action(state)\n","        transitions = P[state][action]\n","        prob, next_state, reward, done = transitions[np.random.choice(len(transitions))]\n","        agent.update_value(state, reward, next_state)\n","        state = next_state\n","\n","# After training\n","print(\"Learned values (4x4 grid form):\")\n","grid_values = agent.values.reshape((4, 4))\n","print(grid_values)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kpTT_QRtZgvr","executionInfo":{"status":"ok","timestamp":1712899899431,"user_tz":-330,"elapsed":370,"user":{"displayName":"Shreya Shree S","userId":"02564406637729607169"}},"outputId":"aea0d1e8-ac7c-4400-b482-b4b6212e3c8f"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Learned values (4x4 grid form):\n","[[0.32197521 0.44965388 0.51307932 0.51733467]\n"," [0.31062721 0.70459221 0.59888972 0.31396492]\n"," [0.30226044 0.14913084 0.12387068 0.12782308]\n"," [0.32834289 0.4562168  0.75125644 0.23785396]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"bhZzeJu2Zif3"},"execution_count":null,"outputs":[]}]}